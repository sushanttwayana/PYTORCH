{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30715,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loss Functions","metadata":{}},{"cell_type":"markdown","source":"**Creating one-hot encoded labels**\n\nOne-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset. This vector only contains zeros and ones. In this exercise, you'll create the one-hot encoded vector of the label y provided.\n\nour dataset contains three classes.\n\nNumPy is already imported as np, and torch.nn.functional as F. The torch package is also imported.","metadata":{}},{"cell_type":"markdown","source":"* Manually create a one-hot encoded vector of the ground truth label y by filling in the NumPy array provided.\n\n* Create a one-hot encoded vector of the ground truth label y using PyTorch.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:06:19.472235Z","iopub.execute_input":"2024-06-06T16:06:19.473353Z","iopub.status.idle":"2024-06-06T16:06:21.639855Z","shell.execute_reply.started":"2024-06-06T16:06:19.473312Z","shell.execute_reply":"2024-06-06T16:06:21.638510Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"y = 1\nnum_classes = 3\n\n# Create the one-hot encoded vector using NumPy\none_hot_numpy = np.array([0, 1, 0])\nprint(one_hot_numpy)\n\n# Create the one-hot encoded vector using PyTorch\none_hot_pytorch = F.one_hot(torch.tensor(y),num_classes)\n\nprint(one_hot_pytorch)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:06:23.408826Z","iopub.execute_input":"2024-06-06T16:06:23.409384Z","iopub.status.idle":"2024-06-06T16:06:23.508215Z","shell.execute_reply.started":"2024-06-06T16:06:23.409338Z","shell.execute_reply":"2024-06-06T16:06:23.506881Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[0 1 0]\ntensor([0, 1, 0])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If you implement a custom dataset, you can make it output the one-hot encoded label directly. Indeed, you can add the one-hot encoding step to the __getitem__ method such that the returned label is already one-hot encoded!","metadata":{}},{"cell_type":"markdown","source":"**Calculating cross entropy loss**\n\nCross entropy loss is the most used loss for classification problems. In this exercise, you will create inputs and calculate cross entropy loss in PyTorch. You are provided with the ground truth label y and a vector of scores predicted by your model.\n\nYou'll start by creating a one-hot encoded vector of the ground truth label y, which is a required step to compare y with the scores predicted by your model. Next, you'll create a cross entropy loss function. Last, you'll call the loss function, which takes scores (model predictions before the final softmax function), and the one-hot encoded ground truth label, as inputs. It outputs a single float, the loss of that sample.\n\n","metadata":{}},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\n\ny = [2]\nscores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n\n# Create a one-hot encoded vector of the label y\none_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n\n# Create the cross entropy loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Calculate the cross entropy loss\nloss = criterion(scores.double(), one_hot_label.double())\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:06:26.699369Z","iopub.execute_input":"2024-06-06T16:06:26.699910Z","iopub.status.idle":"2024-06-06T16:06:26.764327Z","shell.execute_reply.started":"2024-06-06T16:06:26.699872Z","shell.execute_reply":"2024-06-06T16:06:26.763174Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensor(8.0619, dtype=torch.float64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":" This is one of the most commonly used loss functions for classification tasks, where the goal is to predict the probability distribution of a set of target categories or classes.","metadata":{}},{"cell_type":"markdown","source":"# loss and Gradient\n\nRecall that the operation performed by nn.Linear() is to take an input \n and apply the transformation W * X + b \n ,where \n and \n are two tensors (called the weight and bias).\n\nA critical part of training PyTorch models is to calculate gradients of the weight and bias tensors with respect to a loss function.\n\nIn this exercise, you will calculate weight and bias tensor gradients using cross entropy loss and a sample of data.","metadata":{}},{"cell_type":"code","source":"# Weight tensor (2x9)\nweight = torch.tensor([\n    [0.1, -0.2, 0.3, 0.4, -0.5, 0.6, -0.7, 0.8, -0.9],\n    [-0.1, 0.2, -0.3, -0.4, 0.5, -0.6, 0.7, -0.8, 0.9]], requires_grad=True)\n\n# Bias: A 2-element tensor\nbias = torch.tensor([0.1, -0.1], requires_grad=True)\n\n# Example input tensor (1x9), assuming we have 9 features\ninput_tensor = torch.tensor([[0.5, -0.1, 0.2, 0.4, -0.3, 0.6, -0.7, 0.8, -0.9]])\n\n# Calculate the predictions using the weight and bias\npreds = torch.matmul(input_tensor, weight.t()) + bias\n\n# Target: A 1-element tensor containing the class index (not one-hot encoded)\ntarget = torch.tensor([0])\n\n# Define the criterion\ncriterion = nn.CrossEntropyLoss()\n\n# Calculate the loss\nloss = criterion(preds, target)\n\n# Compute the gradients of the loss\nloss.backward()\n\n# Display gradients of the weight and bias tensors\nprint(\"Gradients of weight tensor:\\n\", weight.grad)\nprint(\"Gradients of bias tensor:\\n\", bias.grad)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:16:33.201457Z","iopub.execute_input":"2024-06-06T16:16:33.201891Z","iopub.status.idle":"2024-06-06T16:16:33.237076Z","shell.execute_reply.started":"2024-06-06T16:16:33.201862Z","shell.execute_reply":"2024-06-06T16:16:33.235977Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Gradients of weight tensor:\n tensor([[-0.0017,  0.0003, -0.0007, -0.0014,  0.0010, -0.0020,  0.0024, -0.0027,\n          0.0031],\n        [ 0.0017, -0.0003,  0.0007,  0.0014, -0.0010,  0.0020, -0.0024,  0.0027,\n         -0.0031]])\nGradients of bias tensor:\n tensor([-0.0034,  0.0034])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Accessing the model parameters**\n\nA PyTorch model created with the nn.Sequential() is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network. You won't be accessing the sigmoid.","metadata":{}},{"cell_type":"code","source":"model = nn.Sequential(nn.Linear(16, 8),\n                      nn.Sigmoid(),\n                      nn.Linear(8, 2))\n\n# Access the weight of the first linear layer\nweight_0 = model[0].weight\n\n# Access the bias of the second linear layer\nbias_1 = model[2].bias\n\nprint(\"Weights of the first linear layer:\\n\", weight_0)\nprint(\"Bias of the second linear layer:\\n\", bias_1)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:22:21.552009Z","iopub.execute_input":"2024-06-06T16:22:21.552416Z","iopub.status.idle":"2024-06-06T16:22:21.565621Z","shell.execute_reply.started":"2024-06-06T16:22:21.552382Z","shell.execute_reply":"2024-06-06T16:22:21.564270Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Weights of the first linear layer:\n Parameter containing:\ntensor([[-0.1423, -0.1478, -0.0884, -0.1141, -0.0578,  0.0359,  0.1297,  0.1802,\n          0.0859,  0.2066, -0.1932,  0.1018,  0.0078,  0.1047,  0.1191, -0.0663],\n        [-0.2249,  0.1581,  0.1306, -0.1430,  0.1553,  0.1029,  0.1422, -0.1246,\n         -0.2159,  0.0912,  0.0607, -0.1059,  0.1942,  0.0917, -0.1724,  0.1290],\n        [-0.1499, -0.0373, -0.0353,  0.1264, -0.2333,  0.1531, -0.0538,  0.0040,\n         -0.1968, -0.1421,  0.2244, -0.0176,  0.1909, -0.0598,  0.2307, -0.2306],\n        [-0.0700,  0.0447,  0.0781,  0.0728,  0.1200,  0.1893, -0.1334,  0.1353,\n          0.1712,  0.2275,  0.1818, -0.0209, -0.1674, -0.2108,  0.1260,  0.1216],\n        [-0.1871,  0.1757, -0.0785, -0.0004, -0.1525,  0.2074, -0.1847, -0.1274,\n          0.1373,  0.0351, -0.1874, -0.0103,  0.1444,  0.0548, -0.1626, -0.1775],\n        [ 0.1069,  0.2168, -0.1857,  0.0659, -0.0683,  0.0297, -0.2009, -0.1793,\n          0.1772, -0.1218,  0.0102,  0.2044, -0.1272,  0.2288, -0.1605, -0.1170],\n        [-0.1592, -0.1223, -0.1643, -0.1238, -0.0652,  0.2223,  0.2030,  0.1945,\n          0.0073, -0.1185,  0.2158, -0.0471, -0.0568, -0.0653,  0.2298,  0.0079],\n        [ 0.2368,  0.1845,  0.2237,  0.2202,  0.1304,  0.2255,  0.2480, -0.1091,\n         -0.2430, -0.0816, -0.0332, -0.1224, -0.0595, -0.0857, -0.0494,  0.1258]],\n       requires_grad=True)\nBias of the second linear layer:\n Parameter containing:\ntensor([ 0.0101, -0.2781], requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Updating the weights manually**\n\nNow that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. PyTorch functions can do what you're about to do, but it's helpful to do the work manually at least once, to understand what's going on under the hood.\n\nA neural network of three layers has been created and stored as the model variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, lr, has been chosen to scale the gradients when performing the update.","metadata":{}},{"cell_type":"code","source":"# Define the model\nmodel = nn.Sequential(\n    nn.Linear(16, 8),\n    nn.Sigmoid(),\n    nn.Linear(8, 2)\n)\n\n# Example input tensor with 16 features\ninput_tensor = torch.randn(1, 16)\n\n# Target tensor containing the class index\ntarget = torch.tensor([0])\n\n# Forward pass: compute predictions\npreds = model(input_tensor)\n\n# Define the criterion\ncriterion = nn.CrossEntropyLoss()\n\n# Calculate the loss\nloss = criterion(preds, target)\n\n# Zero the gradients\nmodel.zero_grad()\n\n# Compute the gradients of the loss\nloss.backward()\n\n# Access the weight of each linear layer\nweight0 = model[0].weight\nweight2 = model[2].weight\n\n# Access the gradients of the weight of each linear layer\ngrads0 = weight0.grad\ngrads2 = weight2.grad\n\n# Update the weights using the learning rate and the gradients\nlr = 0.001\n\n# Update the weights manually\nwith torch.no_grad():\n    weight0.data -= lr * grads0\n    weight2.data -= lr * grads2\n\n# Check the updated weights\nprint(\"Updated weights of the first linear layer:\\n\", model[0].weight)\nprint(\"Updated weights of the second linear layer:\\n\", model[2].weight)","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:32:52.955051Z","iopub.execute_input":"2024-06-06T16:32:52.955458Z","iopub.status.idle":"2024-06-06T16:32:52.977222Z","shell.execute_reply.started":"2024-06-06T16:32:52.955408Z","shell.execute_reply":"2024-06-06T16:32:52.976228Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Updated weights of the first linear layer:\n Parameter containing:\ntensor([[-0.2220,  0.0037, -0.1200,  0.0879,  0.2013,  0.0693,  0.1950,  0.0752,\n         -0.1194,  0.1314,  0.0226,  0.2360,  0.1127,  0.0841,  0.2249,  0.1622],\n        [ 0.0961,  0.0146, -0.0532, -0.1262,  0.1990, -0.0035, -0.1104, -0.2489,\n          0.1633, -0.1912,  0.0380,  0.0883,  0.1372, -0.0030,  0.1256,  0.1055],\n        [ 0.0560,  0.0849, -0.1534,  0.0357, -0.0543,  0.1105,  0.1526,  0.1055,\n          0.0965, -0.1959,  0.0978, -0.0289,  0.1946, -0.1748, -0.1036,  0.0813],\n        [ 0.1952, -0.0433,  0.1138, -0.0903, -0.1646, -0.1234, -0.1440, -0.2027,\n          0.1718,  0.2243,  0.0963,  0.0616,  0.0582, -0.2227, -0.2004,  0.0438],\n        [-0.0135,  0.1822, -0.1623, -0.1451, -0.0274,  0.2098,  0.1412, -0.0593,\n         -0.0494, -0.1888, -0.0807, -0.2328, -0.0128, -0.0886,  0.2111, -0.2062],\n        [-0.1569, -0.0406,  0.1758, -0.1298,  0.1087,  0.0017,  0.0657,  0.0516,\n          0.1509, -0.0811, -0.1792, -0.0321, -0.0893, -0.0325,  0.1184,  0.1383],\n        [-0.0293,  0.2249, -0.2500, -0.0728,  0.1694,  0.2203,  0.0610,  0.1063,\n         -0.1066, -0.1363, -0.1143,  0.0953,  0.0755, -0.0119, -0.0243, -0.1232],\n        [ 0.1443, -0.1106,  0.1844,  0.0038,  0.0736,  0.1216, -0.1183, -0.0747,\n          0.0545, -0.1261, -0.0152,  0.2297, -0.0193,  0.1253,  0.2288,  0.2392]],\n       requires_grad=True)\nUpdated weights of the second linear layer:\n Parameter containing:\ntensor([[-0.0264,  0.2687,  0.1799, -0.1532,  0.2680, -0.2217,  0.0669,  0.1036],\n        [ 0.2361,  0.1796,  0.0545,  0.2012, -0.0324,  0.2677, -0.0972, -0.1209]],\n       requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Using the PyTorch optimizer**\n\nIn the previous exercise, you manually updated the weight of a network. You now know what's going on under the hood, but this approach is not scalable to a network of many layers.\n\nThankfully, the PyTorch SGD optimizer does a similar job in a handful of lines of code. In this exercise, you will practice the last step to complete the training loop: updating the weights using a PyTorch optimizer.\n\nA neural network has been created and provided as the model variable. This model was used to run a forward pass and create the tensor of predictions pred. The one-hot encoded tensor is named target and the cross entropy loss function is stored as criterion.\n\ntorch.optim as optim, and torch.nn as nn have already been loaded for you.","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Create the optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\n\n# Calculate the loss\nloss = criterion(preds, target)\n\n# Backpropagation: compute gradients\nloss.backward()\n\n# Update the model's parameters using the optimizer\noptimizer.step()\n\n# If you intend to backpropagate through the graph again, specify retain_graph=True\nloss.backward(retain_graph=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-06T16:40:24.667469Z","iopub.execute_input":"2024-06-06T16:40:24.668146Z","iopub.status.idle":"2024-06-06T16:40:24.740654Z","shell.execute_reply.started":"2024-06-06T16:40:24.668112Z","shell.execute_reply":"2024-06-06T16:40:24.739360Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, target)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Backpropagation: compute gradients\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Update the model's parameters using the optimizer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."],"ename":"RuntimeError","evalue":"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.","output_type":"error"}]}]}