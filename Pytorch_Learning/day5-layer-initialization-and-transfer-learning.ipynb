{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Freeze layers of a model\nYou are about to fine-tune a model on a new task after loading pre-trained weights. The model contains three linear layers. However, because your dataset is small, you only want to train the last linear layer of this model and freeze the first two linear layers.\n\nYou will be using the named_parameters method of the model to list the parameters of the model. Each parameter is described by a name. This name is a string with the following naming convention: x.name where x is the index of the layer.\n\nRemember that a linear layer has two parameters: the weight and the bias\n\n* Use an if statement to determine if the parameter should be frozen or not based on its name.\n* Freeze the parameters of the first two layers of this model.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-06-10T15:34:51.005119Z","iopub.execute_input":"2024-06-10T15:34:51.005791Z","iopub.status.idle":"2024-06-10T15:34:55.370691Z","shell.execute_reply.started":"2024-06-10T15:34:51.005740Z","shell.execute_reply":"2024-06-10T15:34:55.369099Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential(\n    nn.Linear(10, 20),\n    nn.ReLU(),\n    nn.Linear(20, 20),\n    nn.ReLU(),\n    nn.Linear(20, 5)\n)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T12:19:17.652235Z","iopub.execute_input":"2024-06-10T12:19:17.652653Z","iopub.status.idle":"2024-06-10T12:19:17.662987Z","shell.execute_reply.started":"2024-06-10T12:19:17.652621Z","shell.execute_reply":"2024-06-10T12:19:17.661169Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Linear(in_features=10, out_features=20, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=20, out_features=20, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=20, out_features=5, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():    \n  \n    # Check if the parameters belong to the first layer\n    if name == '0.weight' or name == '0.bias':\n      \n        # Freeze the parameters\n        param.requires_grad = False\n  \n    # Check if the parameters belong to the second layer\n    if name == '2.weight' or name == '2.bias':\n      \n        # Freeze the parameters\n        param.requires_grad = False\n        ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T12:20:30.200329Z","iopub.execute_input":"2024-06-10T12:20:30.200733Z","iopub.status.idle":"2024-06-10T12:20:30.209471Z","shell.execute_reply.started":"2024-06-10T12:20:30.200697Z","shell.execute_reply":"2024-06-10T12:20:30.207141Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Verify that the parameters of the first two layers are frozen\nfor name, param in model.named_parameters():\n    print(f\"{name}: requires_grad={param.requires_grad}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T12:20:33.626159Z","iopub.execute_input":"2024-06-10T12:20:33.626574Z","iopub.status.idle":"2024-06-10T12:20:33.633940Z","shell.execute_reply.started":"2024-06-10T12:20:33.626541Z","shell.execute_reply":"2024-06-10T12:20:33.632505Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"0.weight: requires_grad=False\n0.bias: requires_grad=False\n2.weight: requires_grad=False\n2.bias: requires_grad=False\n4.weight: requires_grad=True\n4.bias: requires_grad=True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Choosing which layer to freeze is an empirical process but a good rule of thumb is to start with the first layers and go deeper.","metadata":{}},{"cell_type":"markdown","source":"**Layer initialization**\n\nThe initialization of the weights of a neural network has been the focus of researchers for many years. When training a network, the method used to initialize the weights has a direct impact on the final performance of the network.\n\nAs a machine learning practitioner, you should be able to experiment with different initialization strategies. In this exercise, you are creating a small neural network made of two layers and you are deciding to initialize each layer's weights with the uniform method.\n\n* For each layer (layer0 and layer1), use the uniform initialization method to initialize the weights.","metadata":{}},{"cell_type":"code","source":"layer0 = nn.Linear(4, 8)\nlayer1 = nn.Linear(8, 16)\n\n# Use uniform initialization for layer0 and layer1 weights\nnn.init.uniform_(layer0.weight)\nnn.init.uniform_(layer1.weight)\n\nmodel = nn.Sequential(layer0, layer1)\n\n# Print model layers' weights\nprint(\"Layer 0 weights:\", model[0].weight)\nprint(\"Layer 1 weights:\", model[1].weight)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T12:31:00.683175Z","iopub.execute_input":"2024-06-10T12:31:00.683673Z","iopub.status.idle":"2024-06-10T12:31:00.697762Z","shell.execute_reply.started":"2024-06-10T12:31:00.683638Z","shell.execute_reply":"2024-06-10T12:31:00.696552Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Layer 0 weights: Parameter containing:\ntensor([[0.6928, 0.7653, 0.7003, 0.1282],\n        [0.5271, 0.3602, 0.0314, 0.2038],\n        [0.6965, 0.0276, 0.9807, 0.2103],\n        [0.4551, 0.4438, 0.1325, 0.0393],\n        [0.7576, 0.5620, 0.4499, 0.6103],\n        [0.8542, 0.3954, 0.0239, 0.0258],\n        [0.6765, 0.3739, 0.5181, 0.8950],\n        [0.2078, 0.7522, 0.4847, 0.9370]], requires_grad=True)\nLayer 1 weights: Parameter containing:\ntensor([[0.0470, 0.8998, 0.2830, 0.8920, 0.9998, 0.9316, 0.3186, 0.6546],\n        [0.4531, 0.8117, 0.2813, 0.3909, 0.5903, 0.8229, 0.4271, 0.2445],\n        [0.7003, 0.8792, 0.9795, 0.6034, 0.4508, 0.6155, 0.8383, 0.4566],\n        [0.4285, 0.1114, 0.5640, 0.2554, 0.4231, 0.2409, 0.5597, 0.4028],\n        [0.6415, 0.7968, 0.7504, 0.6868, 0.6673, 0.6035, 0.8084, 0.3392],\n        [0.6228, 0.6406, 0.1622, 0.0280, 0.6484, 0.2807, 0.9071, 0.9459],\n        [0.0753, 0.3577, 0.0779, 0.9153, 0.7900, 0.3867, 0.2820, 0.7087],\n        [0.0323, 0.7941, 0.5514, 0.2151, 0.3968, 0.7261, 0.5166, 0.2845],\n        [0.9263, 0.0449, 0.6329, 0.8095, 0.1881, 0.3819, 0.8159, 0.5036],\n        [0.9523, 0.9853, 0.7359, 0.0049, 0.3534, 0.5694, 0.5256, 0.8328],\n        [0.8482, 0.6780, 0.0691, 0.4370, 0.8700, 0.3627, 0.5463, 0.0078],\n        [0.8461, 0.4587, 0.6908, 0.4777, 0.7340, 0.9663, 0.9538, 0.8606],\n        [0.8030, 0.4043, 0.8968, 0.8314, 0.1467, 0.4569, 0.8043, 0.9707],\n        [0.0680, 0.7940, 0.9830, 0.9108, 0.1702, 0.4738, 0.5596, 0.3028],\n        [0.1358, 0.0139, 0.5538, 0.1799, 0.7954, 0.7258, 0.8070, 0.1991],\n        [0.9441, 0.4041, 0.2056, 0.1158, 0.1841, 0.4495, 0.8266, 0.3570]],\n       requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":" The uniform initialization is one of the many different initialization strategies but they all tend to initialize weights with small values.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}