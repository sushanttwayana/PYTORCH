{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-06-07T16:07:58.737413Z","iopub.execute_input":"2024-06-07T16:07:58.737873Z","iopub.status.idle":"2024-06-07T16:07:58.744268Z","shell.execute_reply.started":"2024-06-07T16:07:58.737833Z","shell.execute_reply":"2024-06-07T16:07:58.742237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Using the MSELoss\n\nRecall that we can't use cross-entropy loss for regression problems. The mean squared error loss (MSELoss) is a common loss function for regression problems. In this exercise, you will practice calculating and observing the loss using NumPy as well as its PyTorch implementation.\n\n* Calculate the MSELoss using NumPy.\n* Create a MSELoss function using PyTorch.\n* Convert y_hat and y to tensors and then float data types, and then use them to calculate MSELoss using PyTorch as mse_pytorch.","metadata":{}},{"cell_type":"code","source":"y_hat = np.array(10)\ny = np.array(1)\n\n# Calculate the MSELoss using NumPy\nmse_numpy = np.mean((y_hat - y)**2)\nprint(\"MSE using NumPy:\", mse_numpy)\n\n# Create the MSELoss function\ncriterion = nn.MSELoss()\n\n# Calculate the MSELoss using the created loss function\nmse_pytorch = criterion(torch.tensor(y).float(), torch.tensor(y_hat).float())\nprint(mse_pytorch)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-07T16:08:01.234336Z","iopub.execute_input":"2024-06-07T16:08:01.234768Z","iopub.status.idle":"2024-06-07T16:08:01.356812Z","shell.execute_reply.started":"2024-06-07T16:08:01.234733Z","shell.execute_reply":"2024-06-07T16:08:01.355466Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"MSE using NumPy: 81.0\ntensor(81.)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"the loss outputs 81, the square of 9, as expected! The MSE loss is also called L2 loss. Another common loss function for regression problem is the mean absolute error loss, also called L1 loss.","metadata":{}},{"cell_type":"markdown","source":"**Writing a training loop**\n\nIn scikit-learn, the whole training loop is contained in the .fit() method. In PyTorch, however, you implement the loop manually. While this provides control over loop's content, it requires a custom implementation.\n\nYou will write a training loop every time you train a deep learning model with PyTorch, which you'll practice in this exercise. The show_results() function provided will display some sample ground truth and the model predictions.\n\n* Write a for loop that iterates over the dataloader; this should be nested within a for loop that iterates over a range equal to the number of epochs.\n\n* Set the gradients of the optimizer to zero.\n* Write the forward pass.\n* Compute the MSE loss value using the criterion() function provided.\n* Compute the gradients.\n* Update the model's parameters.\n\n","metadata":{}},{"cell_type":"code","source":"# Loop over the number of epochs and the dataloader\nfor i in range(num_epochs):\n    for data in dataloader:\n    # Set the gradients to zero\n    optimizer.zero_grad()\n    # Run a forward pass\n    feature, target = data\n    prediction = model(feature)    \n    # Calculate the loss\n    loss = criterion(prediction, target)    \n    # Compute the gradients\n    loss.backward()\n    # Update the model's parameters\n    optimizer.step()\nshow_results(model, dataloader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ground truth salary: 0.078. Predicted salary: 0.210.\n\nGround truth salary: 0.098. Predicted salary: 0.187.\n\nGround truth salary: 0.005. Predicted salary: 0.276.\n\nGround truth salary: 0.293. Predicted salary: 0.171.\n\nGround truth salary: 0.290. Predicted salary: 0.157.\n\nGround truth salary: 0.167. Predicted salary: 0.171.\n\nGround truth salary: 0.169. Predicted salary: 0.171.\n\nGround truth salary: 0.367. Predicted salary: 0.171.\n\nGround truth salary: 0.290. Predicted salary: 0.171.\n\nGround truth salary: 0.417. Predicted salary: 0.205.\n\nGround truth salary: 0.164. Predicted salary: 0.258.\n\nGround truth salary: 0.233. Predicted salary: 0.178.","metadata":{}},{"cell_type":"markdown","source":"# Implementing ReLU\n\nThe rectified linear unit (or ReLU) function is one of the most common activation functions in deep learning.\n\nIt overcomes the training problems linked with the sigmoid function you learned, such as the vanishing gradients problem","metadata":{}},{"cell_type":"code","source":"# Create a ReLU function with PyTorch\nrelu_pytorch = nn.ReLU()\n\n# Apply your ReLU function on x, and calculate gradients\nx = torch.tensor(-1.0, requires_grad=True)\ny = relu_pytorch(x)\ny.backward()\n\n# Print the gradient of the ReLU function for x\ngradient = x.grad\nprint(gradient)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T16:27:39.772573Z","iopub.execute_input":"2024-06-07T16:27:39.773042Z","iopub.status.idle":"2024-06-07T16:27:39.799092Z","shell.execute_reply.started":"2024-06-07T16:27:39.773009Z","shell.execute_reply":"2024-06-07T16:27:39.797964Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"tensor(0.)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice that the input value was -1, and the ReLU function returned zero. Recall from the graph in the video that for negative values of x, the output of ReLU is always zero, and indeed the gradient is zero everywhere because there is no change in the function with respect to any negative value of x.","metadata":{}},{"cell_type":"markdown","source":"**Implementing leaky ReLU**\n\nYou've learned that ReLU is one of the most used activation functions in deep learning. You will find it in modern architecture. However, it does have the inconvenience of outputting null values for negative inputs and therefore, having null gradients. Once an element of the input is negative, it will be set to zero for the rest of the training. Leaky ReLU overcomes this challenge by using a multiplying factor for negative inputs.","metadata":{}},{"cell_type":"code","source":"# Create a leaky relu function in PyTorch\nleaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n\nx = torch.tensor(-2.0)\n# Call the above function on the tensor x\noutput = leaky_relu_pytorch(x)\nprint(output)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-06-07T17:22:13.282691Z","iopub.execute_input":"2024-06-07T17:22:13.283136Z","iopub.status.idle":"2024-06-07T17:22:13.293219Z","shell.execute_reply.started":"2024-06-07T17:22:13.283100Z","shell.execute_reply":"2024-06-07T17:22:13.292048Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"64\n","output_type":"stream"}]}]}