{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8776121,"sourceType":"datasetVersion","datasetId":5274708},{"sourceId":35454000,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T17:46:23.389944Z","iopub.execute_input":"2024-06-24T17:46:23.390391Z","iopub.status.idle":"2024-06-24T17:46:23.397799Z","shell.execute_reply.started":"2024-06-24T17:46:23.390356Z","shell.execute_reply":"2024-06-24T17:46:23.395953Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\n!wget https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip\n\n!wget https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:24:58.933413Z","iopub.execute_input":"2024-06-24T17:24:58.934095Z","iopub.status.idle":"2024-06-24T17:25:03.175916Z","shell.execute_reply.started":"2024-06-24T17:24:58.934063Z","shell.execute_reply":"2024-06-24T17:25:03.174310Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2024-06-24 17:25:00--  https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip\nResolving github.com (github.com)... 140.82.121.3\nConnecting to github.com (github.com)|140.82.121.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip [following]\n--2024-06-24 17:25:00--  https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6462886 (6.2M) [application/zip]\nSaving to: 'images_evaluation.zip'\n\nimages_evaluation.z 100%[===================>]   6.16M  --.-KB/s    in 0.05s   \n\n2024-06-24 17:25:00 (136 MB/s) - 'images_evaluation.zip' saved [6462886/6462886]\n\n--2024-06-24 17:25:01--  https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip\nResolving github.com (github.com)... 140.82.121.3\nConnecting to github.com (github.com)|140.82.121.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip [following]\n--2024-06-24 17:25:02--  https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9464212 (9.0M) [application/zip]\nSaving to: 'images_background.zip'\n\nimages_background.z 100%[===================>]   9.03M  --.-KB/s    in 0.05s   \n\n2024-06-24 17:25:03 (171 MB/s) - 'images_background.zip' saved [9464212/9464212]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\n!unzip -qq images_background.zip\n!unzip -qq images_evaluation.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:25:08.455446Z","iopub.execute_input":"2024-06-24T17:25:08.455890Z","iopub.status.idle":"2024-06-24T17:25:12.643540Z","shell.execute_reply.started":"2024-06-24T17:25:08.455843Z","shell.execute_reply":"2024-06-24T17:25:12.641545Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Two-input dataset\n\nBuilding a multi-input model starts with crafting a custom dataset that can supply all the inputs to the model. In this exercise, you will build the Omniglot dataset that serves triplets consisting of:\n\nThe image of a character to be classified,\nThe one-hot encoded alphabet vector of length 30, with zeros everywhere but for a single one denoting the ID of the alphabet the character comes from,\nThe target label, an integer between 0 and 963.\nYou are provided with train_samples, a list of 3-tuples comprising an image's file path, its alphabet vector, and the target label.\n\n* Assign transform and samples to class attributes with the same names.\n* Implement the .__len()__ method such that it return the number of samples stored in the class' samples attribute.\n* Unpack the sample at index idx assigning its contents to img_path, alphabet, and label.\n* Transform the loaded image with self.transform() and assign it to img_transformed.\n* Nice done! With your implementation of OmniglotDataset ready, you can actually create the dataset and DataLoader, just like you did it before.","metadata":{}},{"cell_type":"code","source":"class OmniglotDataset(Dataset):\n    def __init__(self, transform, samples):\n        # Assign transform and samples to class attributes\n        self.transform = transform\n        self.samples = samples\n\n    def __len__(self):\n        # Return number of samples\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # Unpack the sample at index idx\n        img_path, alphabet, label = self.samples[idx]\n        img = Image.open(img_path).convert('L')\n        # Transform the image \n        img_transformed = self.transform(img)\n        return img_transformed, alphabet, label","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:25:42.767670Z","iopub.execute_input":"2024-06-24T17:25:42.768326Z","iopub.status.idle":"2024-06-24T17:25:42.779263Z","shell.execute_reply.started":"2024-06-24T17:25:42.768270Z","shell.execute_reply":"2024-06-24T17:25:42.777705Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Tensor Concatenation**","metadata":{}},{"cell_type":"code","source":"x = torch.tensor([[1,2,3],])\n\ny = torch.tensor([[4,5,6],])","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:25:45.971911Z","iopub.execute_input":"2024-06-24T17:25:45.972927Z","iopub.status.idle":"2024-06-24T17:25:45.993979Z","shell.execute_reply.started":"2024-06-24T17:25:45.972885Z","shell.execute_reply":"2024-06-24T17:25:45.992769Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Concatenation along axis 0\ntorch.cat((x, y), dim = 0)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:25:48.225109Z","iopub.execute_input":"2024-06-24T17:25:48.225526Z","iopub.status.idle":"2024-06-24T17:25:48.271080Z","shell.execute_reply.started":"2024-06-24T17:25:48.225492Z","shell.execute_reply":"2024-06-24T17:25:48.269861Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 2, 3],\n        [4, 5, 6]])"},"metadata":{}}]},{"cell_type":"code","source":"# Concatenation along axis 1\ntorch.cat((x, y), dim = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:26:00.518201Z","iopub.execute_input":"2024-06-24T17:26:00.518600Z","iopub.status.idle":"2024-06-24T17:26:00.539472Z","shell.execute_reply.started":"2024-06-24T17:26:00.518572Z","shell.execute_reply":"2024-06-24T17:26:00.538315Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 2, 3, 4, 5, 6]])"},"metadata":{}}]},{"cell_type":"markdown","source":"**Two-input model**\n\nWith the data ready, it's time to build the two-input model architecture! To do so, you will set up a model class with the following methods:\n\n.__init__(), in which you will define sub-networks by grouping layers; this is where you define the two layers for processing the two inputs, and the classifier that returns a classification score for each class.\n\nforward(), in which you will pass both inputs through corresponding pre-defined sub-networks, concatenate the outputs, and pass them to the classifier.\n\n* Define image, alphabet and classifier sub-networks as sequential models, assigning them to self.image_layer, self.alphabet_layer and self.classifier, respectively.\n* Pass the image and alphabet through the appropriate model layers.\n* Concatenate the outputs from image and alphabet layers and assign the result to x.","metadata":{}},{"cell_type":"code","source":"# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n#         # Define sub-networks as sequential models\n#         self.image_layer = nn.Sequential(\n#             nn.Conv2d(1, 16, kernel_size=3, padding=1),\n#             nn.MaxPool2d(kernel_size=2),\n#             nn.ELU(),\n#             nn.Flatten(),\n#             nn.Linear(16*32*32, 128)\n#         )\n#         self.alphabet_layer = nn.Sequential(\n#             nn.Linear(30, 8),\n#             nn.ELU(), \n#         )\n#         self.classifier = nn.Sequential(\n#             nn.Linear(128 + 8, 964), \n#         )\n        \n#     def forward(self, x_image, x_alphabet):\n#         # Pass the x_image and x_alphabet through appropriate layers\n#         x_image = self.image_layer(x_image)\n#         x_alphabet = self.alphabet_layer(x_alphabet)\n#         # Concatenate x_image and x_alphabet\n#         x = torch.cat((x_image, x_alphabet), dim=1)\n#         return self.classifier(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:26:05.502258Z","iopub.execute_input":"2024-06-24T17:26:05.502683Z","iopub.status.idle":"2024-06-24T17:26:05.513642Z","shell.execute_reply.started":"2024-06-24T17:26:05.502651Z","shell.execute_reply":"2024-06-24T17:26:05.512159Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Define sub-networks as sequential models\n        self.image_layer = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # Input: [B, 1, 105, 105], Output: [B, 16, 105, 105]\n            nn.MaxPool2d(kernel_size=2),                 # Output: [B, 16, 52, 52]\n            nn.ELU(),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1), # Output: [B, 32, 52, 52]\n            nn.MaxPool2d(kernel_size=2),                 # Output: [B, 32, 26, 26]\n            nn.ELU(),\n            nn.Flatten(),                                # Output: [B, 32*26*26]\n            nn.Linear(32*26*26, 128)                     # Adjust input size based on flattened output\n        )\n        self.alphabet_layer = nn.Sequential(\n            nn.Linear(30, 8),\n            nn.ELU(), \n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(128 + 8, 964), \n        )\n        \n    def forward(self, x_image, x_alphabet):\n        # Ensure x_alphabet is a Tensor and convert to FloatTensor\n        if isinstance(x_alphabet, list):\n            x_alphabet = torch.stack(x_alphabet)\n        x_alphabet = x_alphabet.float()  # Convert to FloatTensor\n        \n        # Pass the x_image and x_alphabet through appropriate layers\n        x_image = self.image_layer(x_image)\n        \n        # Debugging shapes\n#         print(f\"x_image shape after image_layer: {x_image.shape}\")\n#         print(f\"x_alphabet shape before view: {x_alphabet.shape}\")\n        \n        x_alphabet = x_alphabet.view(x_alphabet.size(0), -1)  # Flatten x_alphabet\n        \n        # Debugging shapes\n#         print(f\"x_alphabet shape after view: {x_alphabet.shape}\")\n        \n        x_alphabet = self.alphabet_layer(x_alphabet)\n        \n        # Concatenate x_image and x_alphabet\n        x = torch.cat((x_image, x_alphabet), dim=1)\n        return self.classifier(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:03:06.038180Z","iopub.execute_input":"2024-06-24T18:03:06.038591Z","iopub.status.idle":"2024-06-24T18:03:06.053803Z","shell.execute_reply.started":"2024-06-24T18:03:06.038554Z","shell.execute_reply":"2024-06-24T18:03:06.052390Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"code","source":"import os\n\ndef collect_samples(root_dir, alphabet_map):\n    samples = []\n    \n    # Traverse the root directory\n    for alphabet in sorted(os.listdir(root_dir)):\n        alphabet_path = os.path.join(root_dir, alphabet)\n        \n        if os.path.isdir(alphabet_path):\n            for character in sorted(os.listdir(alphabet_path)):\n                character_path = os.path.join(alphabet_path, character)\n                \n                if os.path.isdir(character_path) and character.startswith('character'):\n                    for image in sorted(os.listdir(character_path)):\n                        image_path = os.path.join(character_path, image)\n                        \n                        # Create one-hot encoded alphabet vector\n                        alphabet_vector = [0] * len(alphabet_map)\n                        alphabet_vector[alphabet_map[alphabet]] = 1\n                        \n                        # Extract label from the character directory name\n                        label = int(character.replace('character', '')) - 1\n                        \n                        samples.append((image_path, alphabet_vector, label))\n    \n    return samples\n\n# Create a map for the alphabets\nroot_dir = '/kaggle/input/omniglot/images_background/images_background'\nalphabet_map = {alphabet: idx for idx, alphabet in enumerate(sorted(os.listdir(root_dir)))}\n\n# Collect samples from images_background and images_evaluation\nbackground_samples = collect_samples(root_dir, alphabet_map)\n# evaluation_samples = collect_samples('/kaggle/input/omniglot/images_evaluation', alphabet_map)\n\n# Combine all samples\nall_samples = background_samples\n\n# Print the number of samples collected for verification\nprint(f\"Number of samples collected: {len(all_samples)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:36:21.144203Z","iopub.execute_input":"2024-06-24T17:36:21.144668Z","iopub.status.idle":"2024-06-24T17:36:22.514660Z","shell.execute_reply.started":"2024-06-24T17:36:21.144632Z","shell.execute_reply":"2024-06-24T17:36:22.513387Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Number of samples collected: 19280\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the transformation for the images\ntransform = transforms.Compose([\n    transforms.Resize((105, 105)),  # Resize images to 105x105\n    transforms.ToTensor(),  # Convert images to tensors\n    transforms.Normalize((0.5,), (0.5,))  # Normalize images to [-1, 1]\n])\n\n# Create the dataset instance\nomniglot_dataset = OmniglotDataset(transform=transform, samples=all_samples)\n\n# Create the DataLoader\ndataloader_train = DataLoader(omniglot_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Iterate through the DataLoader\nfor batch in dataloader_train:\n    images, alphabets, labels = batch\n    \n    # Now you can use images, alphabets, and labels for your training\n    print(images.shape,len(alphabets), labels.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:45:56.776532Z","iopub.execute_input":"2024-06-24T17:45:56.777066Z","iopub.status.idle":"2024-06-24T17:45:57.255677Z","shell.execute_reply.started":"2024-06-24T17:45:56.777026Z","shell.execute_reply":"2024-06-24T17:45:57.254153Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"torch.Size([32, 1, 105, 105]) 30 torch.Size([32])\n","output_type":"stream"}]},{"cell_type":"code","source":"net = Net()\nprint(net)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:03:11.969054Z","iopub.execute_input":"2024-06-24T18:03:11.969505Z","iopub.status.idle":"2024-06-24T18:03:12.005548Z","shell.execute_reply.started":"2024-06-24T18:03:11.969470Z","shell.execute_reply":"2024-06-24T18:03:12.004384Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Net(\n  (image_layer): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): ELU(alpha=1.0)\n    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): ELU(alpha=1.0)\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=21632, out_features=128, bias=True)\n  )\n  (alphabet_layer): Sequential(\n    (0): Linear(in_features=30, out_features=8, bias=True)\n    (1): ELU(alpha=1.0)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=136, out_features=964, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming a batch of images and one-hot encoded alphabets\nimages = torch.randn(32, 1, 105, 105)\nalphabets = torch.randn(32, 30)  # Simulating a tensor of shape (batch_size, 30)\n\n# Forward pass to check dimensions\noutputs = net(images, alphabets)\nprint(outputs.shape)  # Expected output: [32, 964]","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:03:14.299010Z","iopub.execute_input":"2024-06-24T18:03:14.299491Z","iopub.status.idle":"2024-06-24T18:03:14.376089Z","shell.execute_reply.started":"2024-06-24T18:03:14.299449Z","shell.execute_reply":"2024-06-24T18:03:14.374802Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"torch.Size([32, 964])\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1):\n    for img, alpha, labels in dataloader_train:\n        # Check and correct the shape of alpha if necessary\n        if isinstance(alpha, list):\n            alpha = torch.stack(alpha)\n            \n        if alpha.size(0) != img.size(0):\n            alpha = alpha.transpose(0, 1)\n        \n        optimizer.zero_grad()\n        outputs = net(img, alpha)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        print(f\"Loss for epoch {epoch + 1}: {loss.item()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:06:38.541287Z","iopub.execute_input":"2024-06-24T18:06:38.542586Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loss for epoch 1: 6.88652229309082\nLoss for epoch 1: 6.846840858459473\nLoss for epoch 1: 6.838326454162598\nLoss for epoch 1: 6.860101222991943\nLoss for epoch 1: 6.862773895263672\nLoss for epoch 1: 6.861930847167969\nLoss for epoch 1: 6.864839553833008\nLoss for epoch 1: 6.848322868347168\nLoss for epoch 1: 6.874006748199463\nLoss for epoch 1: 6.8832831382751465\nLoss for epoch 1: 6.824965476989746\nLoss for epoch 1: 6.855640888214111\nLoss for epoch 1: 6.849226951599121\nLoss for epoch 1: 6.8916826248168945\nLoss for epoch 1: 6.8698601722717285\nLoss for epoch 1: 6.863776206970215\nLoss for epoch 1: 6.889585018157959\nLoss for epoch 1: 6.881976127624512\nLoss for epoch 1: 6.817143440246582\nLoss for epoch 1: 6.813076019287109\nLoss for epoch 1: 6.850670337677002\nLoss for epoch 1: 6.8541131019592285\nLoss for epoch 1: 6.858354091644287\nLoss for epoch 1: 6.836355209350586\nLoss for epoch 1: 6.843872547149658\nLoss for epoch 1: 6.874183177947998\nLoss for epoch 1: 6.837313652038574\nLoss for epoch 1: 6.89303731918335\nLoss for epoch 1: 6.864494800567627\nLoss for epoch 1: 6.8578200340271\nLoss for epoch 1: 6.846701622009277\nLoss for epoch 1: 6.857943534851074\nLoss for epoch 1: 6.8434858322143555\nLoss for epoch 1: 6.861863613128662\nLoss for epoch 1: 6.892502307891846\nLoss for epoch 1: 6.879577159881592\nLoss for epoch 1: 6.900993347167969\nLoss for epoch 1: 6.814485549926758\nLoss for epoch 1: 6.831486225128174\nLoss for epoch 1: 6.814455509185791\nLoss for epoch 1: 6.890786170959473\nLoss for epoch 1: 6.8341474533081055\nLoss for epoch 1: 6.865451335906982\nLoss for epoch 1: 6.86791467666626\nLoss for epoch 1: 6.852264404296875\nLoss for epoch 1: 6.83725118637085\nLoss for epoch 1: 6.883650302886963\nLoss for epoch 1: 6.8542633056640625\nLoss for epoch 1: 6.823164463043213\nLoss for epoch 1: 6.887100696563721\nLoss for epoch 1: 6.831437110900879\nLoss for epoch 1: 6.816206932067871\nLoss for epoch 1: 6.856272220611572\nLoss for epoch 1: 6.869914531707764\nLoss for epoch 1: 6.842806339263916\nLoss for epoch 1: 6.885383129119873\nLoss for epoch 1: 6.880241870880127\nLoss for epoch 1: 6.875937461853027\nLoss for epoch 1: 6.83348274230957\nLoss for epoch 1: 6.869616508483887\nLoss for epoch 1: 6.8557233810424805\nLoss for epoch 1: 6.891770839691162\nLoss for epoch 1: 6.894843578338623\nLoss for epoch 1: 6.864869594573975\nLoss for epoch 1: 6.86586332321167\nLoss for epoch 1: 6.832735061645508\nLoss for epoch 1: 6.822617530822754\nLoss for epoch 1: 6.847400188446045\nLoss for epoch 1: 6.836592197418213\nLoss for epoch 1: 6.8640241622924805\nLoss for epoch 1: 6.869851589202881\nLoss for epoch 1: 6.876400470733643\nLoss for epoch 1: 6.833032131195068\nLoss for epoch 1: 6.832576751708984\nLoss for epoch 1: 6.894371509552002\nLoss for epoch 1: 6.846179962158203\nLoss for epoch 1: 6.898277759552002\nLoss for epoch 1: 6.888345241546631\nLoss for epoch 1: 6.8455915451049805\nLoss for epoch 1: 6.862436771392822\nLoss for epoch 1: 6.86313533782959\nLoss for epoch 1: 6.879139423370361\nLoss for epoch 1: 6.864834308624268\nLoss for epoch 1: 6.850623607635498\nLoss for epoch 1: 6.909973621368408\nLoss for epoch 1: 6.851495265960693\nLoss for epoch 1: 6.842170238494873\nLoss for epoch 1: 6.893716335296631\nLoss for epoch 1: 6.857946872711182\nLoss for epoch 1: 6.888468265533447\nLoss for epoch 1: 6.832352638244629\nLoss for epoch 1: 6.862001419067383\nLoss for epoch 1: 6.87030553817749\nLoss for epoch 1: 6.858254909515381\nLoss for epoch 1: 6.862953186035156\nLoss for epoch 1: 6.856073379516602\nLoss for epoch 1: 6.844510555267334\nLoss for epoch 1: 6.87446928024292\nLoss for epoch 1: 6.8601179122924805\nLoss for epoch 1: 6.831836223602295\nLoss for epoch 1: 6.881103515625\nLoss for epoch 1: 6.862069606781006\nLoss for epoch 1: 6.877609729766846\nLoss for epoch 1: 6.889665126800537\nLoss for epoch 1: 6.8646464347839355\nLoss for epoch 1: 6.830565929412842\nLoss for epoch 1: 6.8257060050964355\nLoss for epoch 1: 6.850340366363525\nLoss for epoch 1: 6.874574184417725\nLoss for epoch 1: 6.859272480010986\nLoss for epoch 1: 6.846283912658691\nLoss for epoch 1: 6.860213756561279\nLoss for epoch 1: 6.85172176361084\nLoss for epoch 1: 6.8446574211120605\nLoss for epoch 1: 6.8583903312683105\nLoss for epoch 1: 6.877934455871582\nLoss for epoch 1: 6.850371360778809\nLoss for epoch 1: 6.882138252258301\nLoss for epoch 1: 6.8624043464660645\nLoss for epoch 1: 6.848623752593994\nLoss for epoch 1: 6.908051013946533\nLoss for epoch 1: 6.911468029022217\nLoss for epoch 1: 6.82454252243042\nLoss for epoch 1: 6.842251300811768\nLoss for epoch 1: 6.875168800354004\nLoss for epoch 1: 6.847774028778076\nLoss for epoch 1: 6.852876663208008\nLoss for epoch 1: 6.888607978820801\nLoss for epoch 1: 6.849697113037109\nLoss for epoch 1: 6.890695571899414\nLoss for epoch 1: 6.853233337402344\nLoss for epoch 1: 6.866204261779785\nLoss for epoch 1: 6.824402332305908\nLoss for epoch 1: 6.811273574829102\nLoss for epoch 1: 6.81972074508667\nLoss for epoch 1: 6.848518371582031\nLoss for epoch 1: 6.854674816131592\nLoss for epoch 1: 6.829396724700928\nLoss for epoch 1: 6.872077941894531\nLoss for epoch 1: 6.892054080963135\nLoss for epoch 1: 6.904104232788086\nLoss for epoch 1: 6.862148284912109\nLoss for epoch 1: 6.838291645050049\nLoss for epoch 1: 6.885899543762207\nLoss for epoch 1: 6.846511363983154\nLoss for epoch 1: 6.891513824462891\nLoss for epoch 1: 6.844526290893555\nLoss for epoch 1: 6.869170665740967\nLoss for epoch 1: 6.842277526855469\nLoss for epoch 1: 6.858453273773193\nLoss for epoch 1: 6.889075756072998\nLoss for epoch 1: 6.859898090362549\nLoss for epoch 1: 6.842233180999756\nLoss for epoch 1: 6.825294494628906\nLoss for epoch 1: 6.895473480224609\nLoss for epoch 1: 6.8367462158203125\nLoss for epoch 1: 6.8711934089660645\nLoss for epoch 1: 6.82933235168457\nLoss for epoch 1: 6.860408782958984\nLoss for epoch 1: 6.883347988128662\nLoss for epoch 1: 6.825493335723877\nLoss for epoch 1: 6.873723030090332\nLoss for epoch 1: 6.830866813659668\nLoss for epoch 1: 6.880363941192627\nLoss for epoch 1: 6.8589701652526855\nLoss for epoch 1: 6.894505023956299\nLoss for epoch 1: 6.836826324462891\nLoss for epoch 1: 6.903491973876953\nLoss for epoch 1: 6.878691673278809\nLoss for epoch 1: 6.882146835327148\nLoss for epoch 1: 6.852047920227051\nLoss for epoch 1: 6.837014198303223\nLoss for epoch 1: 6.840962886810303\nLoss for epoch 1: 6.893858432769775\nLoss for epoch 1: 6.840773105621338\nLoss for epoch 1: 6.864023685455322\nLoss for epoch 1: 6.843617916107178\nLoss for epoch 1: 6.829633712768555\nLoss for epoch 1: 6.862488269805908\nLoss for epoch 1: 6.882511138916016\nLoss for epoch 1: 6.837728023529053\nLoss for epoch 1: 6.845620155334473\nLoss for epoch 1: 6.840388298034668\nLoss for epoch 1: 6.885186672210693\nLoss for epoch 1: 6.846506118774414\nLoss for epoch 1: 6.8869733810424805\nLoss for epoch 1: 6.832324504852295\nLoss for epoch 1: 6.8797173500061035\nLoss for epoch 1: 6.845942974090576\nLoss for epoch 1: 6.858829975128174\nLoss for epoch 1: 6.889164924621582\nLoss for epoch 1: 6.857664585113525\nLoss for epoch 1: 6.849480628967285\nLoss for epoch 1: 6.869264125823975\nLoss for epoch 1: 6.8501362800598145\nLoss for epoch 1: 6.835264205932617\nLoss for epoch 1: 6.848997592926025\nLoss for epoch 1: 6.85139799118042\nLoss for epoch 1: 6.867186069488525\nLoss for epoch 1: 6.850113868713379\nLoss for epoch 1: 6.842677593231201\nLoss for epoch 1: 6.861215591430664\nLoss for epoch 1: 6.885885715484619\nLoss for epoch 1: 6.83804988861084\nLoss for epoch 1: 6.852099418640137\nLoss for epoch 1: 6.808278560638428\nLoss for epoch 1: 6.874655246734619\nLoss for epoch 1: 6.829921722412109\nLoss for epoch 1: 6.8336501121521\nLoss for epoch 1: 6.8699140548706055\nLoss for epoch 1: 6.8629655838012695\nLoss for epoch 1: 6.851167678833008\nLoss for epoch 1: 6.8308939933776855\nLoss for epoch 1: 6.873477458953857\nLoss for epoch 1: 6.854037284851074\nLoss for epoch 1: 6.861359119415283\nLoss for epoch 1: 6.860752582550049\nLoss for epoch 1: 6.856176853179932\nLoss for epoch 1: 6.840885639190674\nLoss for epoch 1: 6.855323791503906\nLoss for epoch 1: 6.889223575592041\nLoss for epoch 1: 6.867091178894043\nLoss for epoch 1: 6.847219944000244\nLoss for epoch 1: 6.908364772796631\nLoss for epoch 1: 6.839041233062744\nLoss for epoch 1: 6.846731185913086\nLoss for epoch 1: 6.84076452255249\nLoss for epoch 1: 6.897308349609375\nLoss for epoch 1: 6.825255870819092\nLoss for epoch 1: 6.8557305335998535\n","output_type":"stream"}]}]}