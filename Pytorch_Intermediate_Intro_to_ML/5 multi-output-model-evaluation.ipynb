{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Multi-output model evaluation\n\nIn this exercise, you will practice model evaluation for multi-output models. Your task is to write a function called evaluate_model() that takes an alphabet-and-character-predicting model as input, runs the evaluation loop, and prints the model's accuracy in the two tasks.\n\nYou can assume that the function will have access to dataloader_test. \n\n* Define acc_alpha and acc_char as multi-class Accuracy() metrics for the two outputs, alphabets and characters, with the appropriate number of classes each (there are 30 alphabets and 964 characters in the dataset).\n* Define the evaluation loop by iterating over test images, labels_alpha, and labels_char.\n* Inside the for-loop, obtain model results for the test data batch and assign them to outputs_alpha, outputs_char.\n* Update the two accuracy metrics with the current batch's data.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchmetrics import Accuracy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-25T17:40:37.671756Z","iopub.execute_input":"2024-06-25T17:40:37.672473Z","iopub.status.idle":"2024-06-25T17:40:45.169447Z","shell.execute_reply.started":"2024-06-25T17:40:37.672430Z","shell.execute_reply":"2024-06-25T17:40:45.168517Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model):\n    # Define accuracy metrics\n    acc_alpha = Accuracy(task=\"multiclass\", num_classes=30)\n    acc_char = Accuracy(task=\"multiclass\", num_classes=964)\n\n    model.eval()\n    with torch.no_grad():\n        for images, labels_alpha, labels_char in dataloader_test:\n            # Obtain model outputs\n            outputs_alpha, outputs_char = model(images)\n            _, pred_alpha = torch.max(outputs_alpha, 1)\n            _, pred_char = torch.max(outputs_char, 1)\n\t\t\t# Update both accuracy metrics\n            acc_alpha(pred_alpha, labels_alpha)\n            acc_char(pred_char, labels_char)\n    \n    print(f\"Alphabet: {acc_alpha.compute()}\")\n    print(f\"Character: {acc_char.compute()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Notice how the model with 90% of its focus on alphabet recognition (char_weight=0.1) does very poorly on the character task. As we increase char_weight to 0.5, the alphabet accuracy drops slightly due to the increased focus on characters, but when it reaches char_weight=0.9, the alphabet accuracy increases slightly with the character accuracy, highlighting the synergy between the tasks.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}