{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8776121,"sourceType":"datasetVersion","datasetId":5274708},{"sourceId":35454000,"sourceType":"kernelVersion"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-25T16:50:45.597443Z","iopub.execute_input":"2024-06-25T16:50:45.597842Z","iopub.status.idle":"2024-06-25T16:50:52.775701Z","shell.execute_reply.started":"2024-06-25T16:50:45.597809Z","shell.execute_reply":"2024-06-25T16:50:52.774573Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\n!wget https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip\n\n!wget https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:24:58.933413Z","iopub.execute_input":"2024-06-24T17:24:58.934095Z","iopub.status.idle":"2024-06-24T17:25:03.175916Z","shell.execute_reply.started":"2024-06-24T17:24:58.934063Z","shell.execute_reply":"2024-06-24T17:25:03.174310Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2024-06-24 17:25:00--  https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip\nResolving github.com (github.com)... 140.82.121.3\nConnecting to github.com (github.com)|140.82.121.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip [following]\n--2024-06-24 17:25:00--  https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_evaluation.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 6462886 (6.2M) [application/zip]\nSaving to: 'images_evaluation.zip'\n\nimages_evaluation.z 100%[===================>]   6.16M  --.-KB/s    in 0.05s   \n\n2024-06-24 17:25:00 (136 MB/s) - 'images_evaluation.zip' saved [6462886/6462886]\n\n--2024-06-24 17:25:01--  https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip\nResolving github.com (github.com)... 140.82.121.3\nConnecting to github.com (github.com)|140.82.121.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip [following]\n--2024-06-24 17:25:02--  https://raw.githubusercontent.com/brendenlake/omniglot/master/python/images_background.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9464212 (9.0M) [application/zip]\nSaving to: 'images_background.zip'\n\nimages_background.z 100%[===================>]   9.03M  --.-KB/s    in 0.05s   \n\n2024-06-24 17:25:03 (171 MB/s) - 'images_background.zip' saved [9464212/9464212]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"\n!unzip -qq images_background.zip\n!unzip -qq images_evaluation.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:25:08.455446Z","iopub.execute_input":"2024-06-24T17:25:08.455890Z","iopub.status.idle":"2024-06-24T17:25:12.643540Z","shell.execute_reply.started":"2024-06-24T17:25:08.455843Z","shell.execute_reply":"2024-06-24T17:25:12.641545Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Two-input dataset\n\nBuilding a multi-input model starts with crafting a custom dataset that can supply all the inputs to the model. In this exercise, you will build the Omniglot dataset that serves triplets consisting of:\n\nThe image of a character to be classified,\nThe one-hot encoded alphabet vector of length 30, with zeros everywhere but for a single one denoting the ID of the alphabet the character comes from,\nThe target label, an integer between 0 and 963.\nYou are provided with train_samples, a list of 3-tuples comprising an image's file path, its alphabet vector, and the target label.\n\n* Assign transform and samples to class attributes with the same names.\n* Implement the .__len()__ method such that it return the number of samples stored in the class' samples attribute.\n* Unpack the sample at index idx assigning its contents to img_path, alphabet, and label.\n* Transform the loaded image with self.transform() and assign it to img_transformed.\n* Nice done! With your implementation of OmniglotDataset ready, you can actually create the dataset and DataLoader, just like you did it before.","metadata":{}},{"cell_type":"code","source":"class OmniglotDataset(Dataset):\n    def __init__(self, transform, samples):\n        # Assign transform and samples to class attributes\n        self.transform = transform\n        self.samples = samples\n\n    def __len__(self):\n        # Return number of samples\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # Unpack the sample at index idx\n        img_path, alphabet, label = self.samples[idx]\n        img = Image.open(img_path).convert('L')\n        # Transform the image \n        img_transformed = self.transform(img)\n        return img_transformed, alphabet, label","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:51:22.584985Z","iopub.execute_input":"2024-06-25T16:51:22.585402Z","iopub.status.idle":"2024-06-25T16:51:22.593067Z","shell.execute_reply.started":"2024-06-25T16:51:22.585364Z","shell.execute_reply":"2024-06-25T16:51:22.591896Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Tensor Concatenation**","metadata":{}},{"cell_type":"code","source":"x = torch.tensor([[1,2,3],])\n\ny = torch.tensor([[4,5,6],])","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:25:45.971911Z","iopub.execute_input":"2024-06-24T17:25:45.972927Z","iopub.status.idle":"2024-06-24T17:25:45.993979Z","shell.execute_reply.started":"2024-06-24T17:25:45.972885Z","shell.execute_reply":"2024-06-24T17:25:45.992769Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Concatenation along axis 0\ntorch.cat((x, y), dim = 0)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:25:48.225109Z","iopub.execute_input":"2024-06-24T17:25:48.225526Z","iopub.status.idle":"2024-06-24T17:25:48.271080Z","shell.execute_reply.started":"2024-06-24T17:25:48.225492Z","shell.execute_reply":"2024-06-24T17:25:48.269861Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 2, 3],\n        [4, 5, 6]])"},"metadata":{}}]},{"cell_type":"code","source":"# Concatenation along axis 1\ntorch.cat((x, y), dim = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:26:00.518201Z","iopub.execute_input":"2024-06-24T17:26:00.518600Z","iopub.status.idle":"2024-06-24T17:26:00.539472Z","shell.execute_reply.started":"2024-06-24T17:26:00.518572Z","shell.execute_reply":"2024-06-24T17:26:00.538315Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 2, 3, 4, 5, 6]])"},"metadata":{}}]},{"cell_type":"markdown","source":"**Two-input model**\n\nWith the data ready, it's time to build the two-input model architecture! To do so, you will set up a model class with the following methods:\n\n.__init__(), in which you will define sub-networks by grouping layers; this is where you define the two layers for processing the two inputs, and the classifier that returns a classification score for each class.\n\nforward(), in which you will pass both inputs through corresponding pre-defined sub-networks, concatenate the outputs, and pass them to the classifier.\n\n* Define image, alphabet and classifier sub-networks as sequential models, assigning them to self.image_layer, self.alphabet_layer and self.classifier, respectively.\n* Pass the image and alphabet through the appropriate model layers.\n* Concatenate the outputs from image and alphabet layers and assign the result to x.","metadata":{}},{"cell_type":"code","source":"# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n#         # Define sub-networks as sequential models\n#         self.image_layer = nn.Sequential(\n#             nn.Conv2d(1, 16, kernel_size=3, padding=1),\n#             nn.MaxPool2d(kernel_size=2),\n#             nn.ELU(),\n#             nn.Flatten(),\n#             nn.Linear(16*32*32, 128)\n#         )\n#         self.alphabet_layer = nn.Sequential(\n#             nn.Linear(30, 8),\n#             nn.ELU(), \n#         )\n#         self.classifier = nn.Sequential(\n#             nn.Linear(128 + 8, 964), \n#         )\n        \n#     def forward(self, x_image, x_alphabet):\n#         # Pass the x_image and x_alphabet through appropriate layers\n#         x_image = self.image_layer(x_image)\n#         x_alphabet = self.alphabet_layer(x_alphabet)\n#         # Concatenate x_image and x_alphabet\n#         x = torch.cat((x_image, x_alphabet), dim=1)\n#         return self.classifier(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:26:05.502258Z","iopub.execute_input":"2024-06-24T17:26:05.502683Z","iopub.status.idle":"2024-06-24T17:26:05.513642Z","shell.execute_reply.started":"2024-06-24T17:26:05.502651Z","shell.execute_reply":"2024-06-24T17:26:05.512159Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Define sub-networks as sequential models\n        self.image_layer = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # Input: [B, 1, 105, 105], Output: [B, 16, 105, 105]\n            nn.MaxPool2d(kernel_size=2),                 # Output: [B, 16, 52, 52]\n            nn.ELU(),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1), # Output: [B, 32, 52, 52]\n            nn.MaxPool2d(kernel_size=2),                 # Output: [B, 32, 26, 26]\n            nn.ELU(),\n            nn.Flatten(),                                # Output: [B, 32*26*26]\n            nn.Linear(32*26*26, 128)                     # Adjust input size based on flattened output\n        )\n        self.alphabet_layer = nn.Sequential(\n            nn.Linear(30, 8),\n            nn.ELU(), \n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(128 + 8, 964), \n        )\n        \n    def forward(self, x_image, x_alphabet):\n        # Ensure x_alphabet is a Tensor and convert to FloatTensor\n        if isinstance(x_alphabet, list):\n            x_alphabet = torch.stack(x_alphabet)\n        x_alphabet = x_alphabet.float()  # Convert to FloatTensor\n        \n        # Pass the x_image and x_alphabet through appropriate layers\n        x_image = self.image_layer(x_image)\n        \n        # Debugging shapes\n#         print(f\"x_image shape after image_layer: {x_image.shape}\")\n#         print(f\"x_alphabet shape before view: {x_alphabet.shape}\")\n        \n        x_alphabet = x_alphabet.view(x_alphabet.size(0), -1)  # Flatten x_alphabet\n        \n        # Debugging shapes\n#         print(f\"x_alphabet shape after view: {x_alphabet.shape}\")\n        \n        x_alphabet = self.alphabet_layer(x_alphabet)\n        \n        # Concatenate x_image and x_alphabet\n        x = torch.cat((x_image, x_alphabet), dim=1)\n        return self.classifier(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:03:06.038180Z","iopub.execute_input":"2024-06-24T18:03:06.038591Z","iopub.status.idle":"2024-06-24T18:03:06.053803Z","shell.execute_reply.started":"2024-06-24T18:03:06.038554Z","shell.execute_reply":"2024-06-24T18:03:06.052390Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"code","source":"import os\n\ndef collect_samples(root_dir, alphabet_map):\n    samples = []\n    \n    # Traverse the root directory\n    for alphabet in sorted(os.listdir(root_dir)):\n        alphabet_path = os.path.join(root_dir, alphabet)\n        \n        if os.path.isdir(alphabet_path):\n            for character in sorted(os.listdir(alphabet_path)):\n                character_path = os.path.join(alphabet_path, character)\n                \n                if os.path.isdir(character_path) and character.startswith('character'):\n                    for image in sorted(os.listdir(character_path)):\n                        image_path = os.path.join(character_path, image)\n                        \n                        # Create one-hot encoded alphabet vector\n                        alphabet_vector = [0] * len(alphabet_map)\n                        alphabet_vector[alphabet_map[alphabet]] = 1\n                        \n                        # Extract label from the character directory name\n                        label = int(character.replace('character', '')) - 1\n                        \n                        samples.append((image_path, alphabet_vector, label))\n    \n    return samples\n\n# Create a map for the alphabets\nroot_dir = '/kaggle/input/omniglot/images_background/images_background'\nalphabet_map = {alphabet: idx for idx, alphabet in enumerate(sorted(os.listdir(root_dir)))}\n\n# Collect samples from images_background and images_evaluation\nbackground_samples = collect_samples(root_dir, alphabet_map)\n# evaluation_samples = collect_samples('/kaggle/input/omniglot/images_evaluation', alphabet_map)\n\n# Combine all samples\nall_samples = background_samples\n\n# Print the number of samples collected for verification\nprint(f\"Number of samples collected: {len(all_samples)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-24T17:36:21.144203Z","iopub.execute_input":"2024-06-24T17:36:21.144668Z","iopub.status.idle":"2024-06-24T17:36:22.514660Z","shell.execute_reply.started":"2024-06-24T17:36:21.144632Z","shell.execute_reply":"2024-06-24T17:36:22.513387Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Number of samples collected: 19280\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the transformation for the images\ntransform = transforms.Compose([\n    transforms.Resize((105, 105)),  # Resize images to 105x105\n    transforms.ToTensor(),  # Convert images to tensors\n    transforms.Normalize((0.5,), (0.5,))  # Normalize images to [-1, 1]\n])\n\n# Create the dataset instance\nomniglot_dataset = OmniglotDataset(transform=transform, samples=all_samples)\n\n# Create the DataLoader\ndataloader_train = DataLoader(omniglot_dataset, batch_size=32, shuffle=True, num_workers=4)\n\n# Iterate through the DataLoader\nfor batch in dataloader_train:\n    images, alphabets, labels = batch\n    \n    # Now you can use images, alphabets, and labels for your training\n    print(images.shape,len(alphabets), labels.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-06-25T16:51:28.829009Z","iopub.execute_input":"2024-06-25T16:51:28.829425Z","iopub.status.idle":"2024-06-25T16:51:28.871485Z","shell.execute_reply.started":"2024-06-25T16:51:28.829392Z","shell.execute_reply":"2024-06-25T16:51:28.870137Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m105\u001b[39m, \u001b[38;5;241m105\u001b[39m)),  \u001b[38;5;66;03m# Resize images to 105x105\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),  \u001b[38;5;66;03m# Convert images to tensors\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m,), (\u001b[38;5;241m0.5\u001b[39m,))  \u001b[38;5;66;03m# Normalize images to [-1, 1]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create the dataset instance\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m omniglot_dataset \u001b[38;5;241m=\u001b[39m OmniglotDataset(transform\u001b[38;5;241m=\u001b[39mtransform, samples\u001b[38;5;241m=\u001b[39m\u001b[43mall_samples\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create the DataLoader\u001b[39;00m\n\u001b[1;32m     12\u001b[0m dataloader_train \u001b[38;5;241m=\u001b[39m DataLoader(omniglot_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'all_samples' is not defined"],"ename":"NameError","evalue":"name 'all_samples' is not defined","output_type":"error"}]},{"cell_type":"code","source":"net = Net()\nprint(net)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:03:11.969054Z","iopub.execute_input":"2024-06-24T18:03:11.969505Z","iopub.status.idle":"2024-06-24T18:03:12.005548Z","shell.execute_reply.started":"2024-06-24T18:03:11.969470Z","shell.execute_reply":"2024-06-24T18:03:12.004384Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Net(\n  (image_layer): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (2): ELU(alpha=1.0)\n    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): ELU(alpha=1.0)\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=21632, out_features=128, bias=True)\n  )\n  (alphabet_layer): Sequential(\n    (0): Linear(in_features=30, out_features=8, bias=True)\n    (1): ELU(alpha=1.0)\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=136, out_features=964, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming a batch of images and one-hot encoded alphabets\nimages = torch.randn(32, 1, 105, 105)\nalphabets = torch.randn(32, 30)  # Simulating a tensor of shape (batch_size, 30)\n\n# Forward pass to check dimensions\noutputs = net(images, alphabets)\nprint(outputs.shape)  # Expected output: [32, 964]","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:03:14.299010Z","iopub.execute_input":"2024-06-24T18:03:14.299491Z","iopub.status.idle":"2024-06-24T18:03:14.376089Z","shell.execute_reply.started":"2024-06-24T18:03:14.299449Z","shell.execute_reply":"2024-06-24T18:03:14.374802Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"torch.Size([32, 964])\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(1):\n    for img, alpha, labels in dataloader_train:\n        # Check and correct the shape of alpha if necessary\n        if isinstance(alpha, list):\n            alpha = torch.stack(alpha)\n            \n        if alpha.size(0) != img.size(0):\n            alpha = alpha.transpose(0, 1)\n        \n        optimizer.zero_grad()\n        outputs = net(img, alpha)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \nprint(f\"Loss for epoch {epoch + 1}: {loss.item()}\")\n       ","metadata":{"execution":{"iopub.status.busy":"2024-06-24T18:17:56.295346Z","iopub.execute_input":"2024-06-24T18:17:56.295820Z","iopub.status.idle":"2024-06-24T18:19:16.794995Z","shell.execute_reply.started":"2024-06-24T18:17:56.295781Z","shell.execute_reply":"2024-06-24T18:19:16.793608Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Loss for epoch 1: 6.856453895568848\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Multi_Outut Models","metadata":{}},{"cell_type":"markdown","source":"**Two-output Dataset and DataLoader**\n\nIn this and the following exercises, you will build a two-output model to predict both the character and the alphabet it comes from based on the character's image. As always, you will start with getting the data ready.\n\nThe OmniglotDataset class you have created before is available for you to use along with updated samples. Let's use it to build the Dataset and the DataLoader.\n\n","metadata":{}},{"cell_type":"code","source":"class OmniglotDataset(Dataset):\n    def __init__(self, transform, samples):\n        # Assign transform and samples to class attributes\n        self.transform = transform\n        self.samples = samples\n\n    def __len__(self):\n        # Return number of samples\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        # Unpack the sample at index idx\n        img_path, alphabet, label =  self.samples[idx]\n        img = Image.open(img_path).convert('L')\n        # Transform the image \n        img_transformed = self.transform(img)\n        return img_transformed, alphabet, label","metadata":{"execution":{"iopub.status.busy":"2024-06-25T17:09:40.013354Z","iopub.execute_input":"2024-06-25T17:09:40.013819Z","iopub.status.idle":"2024-06-25T17:09:40.021137Z","shell.execute_reply.started":"2024-06-25T17:09:40.013786Z","shell.execute_reply":"2024-06-25T17:09:40.020005Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Two-output model architecture**\n\nIn this exercise, you will construct a multi-output neural network architecture capable of predicting the character and the alphabet.\n\nRecall the general structure: in the .__init__() method, you define layers to be used in the forward pass later. In the forward() method, you will first pass the input image through a couple of layers to obtain its embedding, which in turn is fed into two separate classifier layers, one for each output.\n\n* Define self.classifier_alpha and self.classifier_char as linear layers with input shapes matching the output of image_layer, and output shapes corresponding to the number of alphabets (30) and the number of characters (964), respectively.\n* Pass the image embedding x_image separately through each of the classifiers, assigning the results to output_alpha and output_char, respectively, and return them in this order.","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # Define sub-networks as sequential models\n        self.image_layer = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),  \n            nn.MaxPool2d(kernel_size=2),                 \n            nn.Conv2d(16, 32, kernel_size=3, padding=1), # Output: [B, 32, 52, 52]\n            nn.MaxPool2d(kernel_size=2),                 # Output: [B, 32, 26, 26]\n            nn.ELU(),\n            nn.Flatten(),                                # Output: [B, 32*26*26]\n            nn.Linear(32*26*26, 128)                     # Adjust input size based on flattened output\n        )\n        \n        self.classifier_alpha = nn.Linear(128, 30)\n        self.classifer_char = nn.Linear(128, 964)\n        \n    def forward(self, x_image, x_alphabet):\n        # Ensure x_alphabet is a Tensor and convert to FloatTensor\n        if isinstance(x_alphabet, list):\n            x_alphabet = torch.stack(x_alphabet)\n        x_alphabet = x_alphabet.float()  # Convert to FloatTensor\n        \n        # Pass the x_image and x_alphabet through appropriate layers\n        x_image = self.image_layer(x_image)\n        \n        output_alpha = self.classifier_alpha(x_image)\n        output_char = self.classifer_char(x_image)\n        return ouput_alpha, output_char","metadata":{"execution":{"iopub.status.busy":"2024-06-25T17:09:42.937713Z","iopub.execute_input":"2024-06-25T17:09:42.938575Z","iopub.status.idle":"2024-06-25T17:09:42.948429Z","shell.execute_reply.started":"2024-06-25T17:09:42.938538Z","shell.execute_reply":"2024-06-25T17:09:42.947215Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Print the sample at index 100\nprint(samples[100])\n\n# Create dataset_train\ndataset_train = OmniglotDataset(\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n      \ttransforms.Resize((64, 64)),\n    ]),\n    samples=samples,\n)\n\n# Create dataloader_train\ndataloader_train = DataLoader(\n    dataset_train, shuffle=True, batch_size=32,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice how samples now contain, next to the image path, the target labels for the character and the alphabet. In the next exercise, you will examine the architecture of the two-output model.","metadata":{}},{"cell_type":"markdown","source":"**Training multi-output models**\n\nWhen training models with multiple outputs, it is crucial to ensure that the loss function is defined correctly.\n\nIn this case, the model produces two outputs: predictions for the alphabet and the character. For each of these, there are corresponding ground truth labels, which will allow you to calculate two separate losses: one incurred from incorrect alphabet classifications, and the other from incorrect character classification. Since in both cases you are dealing with a multi-label classification task, the Cross-Entropy loss can be applied each time.\n\nGradient descent can optimize only one loss function, however. You will thus define the total loss as the sum of alphabet and character losses.\n\n1. Calculate the alphabet classification loss and assign it to loss_alpha.\n2. Calculate the character classification loss and assign it to loss_char.\n3. Compute the total loss as the sum of the two partial losses and assign it to loss.","metadata":{}},{"cell_type":"code","source":"net = Net2()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.05)\n\nfor epoch in range(1):\n    for images, labels_alpha, labels_char in dataloader_train:\n        optimizer.zero_grad()\n        outputs_alpha, outputs_char = net(images)\n        # Compute alphabet classification loss\n        loss_alpha = criterion(outputs_alpha, labels_alpha)\n        # Compute character classification loss\n        loss_char = criterion(outputs_char, labels_char)\n        # Compute total loss\n        loss = loss_alpha + loss_char\n        loss.backward()\n        optimizer.step()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Defining the total loss as the sum of the two task-specific losses is a simple way to obtain the single optimization objective required by gradient descent. There are, however, other ways to combine the partial losses. ","metadata":{}}]}