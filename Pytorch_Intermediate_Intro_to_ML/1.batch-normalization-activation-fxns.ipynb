{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initialization and activation\n\nThe problems of unstable (vanishing or exploding) gradients are a challenge that often arises in training deep neural networks. In this and the following exercises, you will expand the model architecture that you built for the water potability classification task to make it more immune to those problems.\n\nAs a first step, you'll improve the weights initialization by using He (Kaiming) initialization strategy. To do so, you will need to call the proper initializer from the torch.nn.init module, which has been imported for you as init. Next, you will update the activations functions from the default ReLU to the often better ELU.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T16:19:37.871029Z","iopub.execute_input":"2024-06-13T16:19:37.871432Z","iopub.status.idle":"2024-06-13T16:19:39.186677Z","shell.execute_reply.started":"2024-06-13T16:19:37.871398Z","shell.execute_reply":"2024-06-13T16:19:39.185436Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"* Call the He (Kaiming) initializer on the weight attribute of the second layer, fc2, similarly to how it's done for fc1.\n* Call the He (Kaiming) initializer on the weight attribute of the third layer, fc3, accounting for the different activation function used in the final layer.\n* Update the activation functions in the forward() method from relu to elu.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:20:21.332678Z","iopub.execute_input":"2024-06-13T16:20:21.338285Z","iopub.status.idle":"2024-06-13T16:20:21.352766Z","shell.execute_reply.started":"2024-06-13T16:20:21.338203Z","shell.execute_reply":"2024-06-13T16:20:21.349721Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(9, 16)\n        self.fc2 = nn.Linear(16, 8)\n        self.fc3 = nn.Linear(8, 1)\n        \n        # Apply He initialization\n        init.kaiming_uniform_(self.fc1.weight)\n        init.kaiming_uniform_(self.fc2.weight)\n        init.kaiming_uniform_(self.fc3.weight, nonlinearity = \"sigmoid\")\n\n    def forward(self, x):\n        # Update ReLU activation to ELU\n        x = nn.functional.elu(self.fc1(x))\n        x = nn.functional.elu(self.fc2(x))\n        x = nn.functional.sigmoid(self.fc3(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:20:25.842090Z","iopub.execute_input":"2024-06-13T16:20:25.842641Z","iopub.status.idle":"2024-06-13T16:20:25.851967Z","shell.execute_reply.started":"2024-06-13T16:20:25.842599Z","shell.execute_reply":"2024-06-13T16:20:25.850405Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**Batch Normalization**\n\nAs a final improvement to the model architecture, let's add the batch normalization layer after each of the two linear layers. The batch norm trick tends to accelerate training convergence and protects the model from vanishing and exploding gradients issues.\n\nBoth torch.nn and torch.nn.init have already been imported for you as nn and init, respectively. Once you implement the change in the model architecture, be ready to answer a short question on how batch normalization works!","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(9, 16)\n        self.fc2 = nn.Linear(16, 8)\n        self.fc3 = nn.Linear(8, 1)\n        # Add two batch normalization layers\n        self.bn1 = nn.BatchNorm1d(16)\n        self.bn2 = nn.BatchNorm1d(8)\n        \n        init.kaiming_uniform_(self.fc1.weight)\n        init.kaiming_uniform_(self.fc2.weight)\n        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)\n        x = nn.functional.elu(x)\n\n        # Pass x through the second set of layers\n        x = self.fc2(x)\n        x = self.bn2(x)\n        x = nn.functional.elu(x)\n\n        x = nn.functional.sigmoid(self.fc3(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-13T16:39:37.323173Z","iopub.execute_input":"2024-06-13T16:39:37.324230Z","iopub.status.idle":"2024-06-13T16:39:37.334905Z","shell.execute_reply.started":"2024-06-13T16:39:37.324192Z","shell.execute_reply":"2024-06-13T16:39:37.333728Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"In the forward() method, pass x through the second set of layers: the linear layer, the batch norm layer, and the activations, similarly to how it's done for the first set of layers.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By learning how to optimally re-scale the next layer's inputs, batch normalization mitigates the unstable gradients problems! Congratulations on finishing Chapter 1 of this course! See you in Chapter 2, where we will build convolutional neural networks (CNNs)â€”models designed for image processing!","metadata":{}}]}