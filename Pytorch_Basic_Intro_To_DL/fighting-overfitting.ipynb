{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experimenting with dropout\nThe dropout layer randomly zeroes out elements of the input tensor. Doing so helps fight overfitting. In this exercise, you'll create a small neural network with at least two linear layers, two dropout layers, and two activation functions.\n\n* Create a small neural network with one linear layer, one ReLU function, and one dropout layer, in that order.\nThe model should take input_tensor as input and return an output of size 16.\n* Using the same neural network, set the probability of zeroing out elements in the dropout layer to 0.8.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T16:52:19.301794Z","iopub.execute_input":"2024-06-12T16:52:19.302309Z","iopub.status.idle":"2024-06-12T16:52:22.925846Z","shell.execute_reply.started":"2024-06-12T16:52:19.302270Z","shell.execute_reply":"2024-06-12T16:52:22.924636Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Create a small neural network\ninput_tensor = torch.randn(1, 3072)  \nmodel = nn.Sequential(nn.Linear(3072,16), nn.ReLU(), nn.Dropout())\nmodel(input_tensor)\n\nprint(model)\noutput = model(input_tensor)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:53:37.160689Z","iopub.execute_input":"2024-06-12T16:53:37.161111Z","iopub.status.idle":"2024-06-12T16:53:37.175668Z","shell.execute_reply.started":"2024-06-12T16:53:37.161078Z","shell.execute_reply":"2024-06-12T16:53:37.174417Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Sequential(\n  (0): Linear(in_features=3072, out_features=16, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.5, inplace=False)\n)\ntensor([[0.0000, 0.0000, 0.0593, 0.0000, 0.4999, 0.0000, 0.0000, 0.4336, 0.0000,\n         0.0000, 0.0000, 0.6064, 0.0000, 0.0000, 0.0000, 0.0000]],\n       grad_fn=<MulBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Using the same model, set the dropout probability to 0.8\nmodel = nn.Sequential(nn.Linear(3072,16), nn.ReLU(), nn.Dropout(p = 0.8))\nmodel(input_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T16:54:00.795173Z","iopub.execute_input":"2024-06-12T16:54:00.795939Z","iopub.status.idle":"2024-06-12T16:54:00.809298Z","shell.execute_reply.started":"2024-06-12T16:54:00.795908Z","shell.execute_reply":"2024-06-12T16:54:00.808057Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.0000, 0.0000, 5.0301, 0.0000, 0.0000, 0.0000]],\n       grad_fn=<MulBackward0>)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Implementing random search\n\nHyperparameter search is a computationally costly approach to experiment with different hyperparameter values. However, it can lead to performance improvements. In this exercise, you will implement a random search algorithm.\n\nYou will randomly sample 10 values of the learning rate and momentum from the uniform distribution. To do so, you will use the np.random.uniform() function.\n\n* Randomly sample a learning rate factor between 2 and 4 so that the learning rate (lr) is bounded between \n and \n.\n* Randomly sample a momentum between 0.85 and 0.99.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nvalues = []\nfor idx in range(10):\n    # Randomly sample a learning rate factor between 2 and 4\n    factor = np.random.uniform(2,4)\n    lr = 10 ** -factor\n    \n    # Randomly select a momentum between 0.85 and 0.99\n    momentum = np.random.uniform(0.85, 0.99)\n    \n    values.append((lr, momentum))\n\nprint(lr)\nprint(momentum)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T17:10:17.385056Z","iopub.execute_input":"2024-06-12T17:10:17.385453Z","iopub.status.idle":"2024-06-12T17:10:17.393445Z","shell.execute_reply.started":"2024-06-12T17:10:17.385423Z","shell.execute_reply":"2024-06-12T17:10:17.392230Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"0.0007090064425725132\n0.9208787829229824\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Random search is a great way to fine-tune your hyperparameters. Upper and lower bounds should be carefully chosen to not waste computational power.","metadata":{}}]}